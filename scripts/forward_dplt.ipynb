{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward dPL Multi-layer Terzaghi's 1D Consolidation Model\n",
    "\n",
    "-- Land subsidence project: 15 March 2025 --\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Preperations\n",
    "\n",
    "Essentially, the internals of what will eventually be the data loader for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HydroDL2 not found. Continuing without it.\n"
     ]
    }
   ],
   "source": [
    "### Load in data\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../dMG')  # Add the dMG root directory.\n",
    "sys.path.append(os.path.abspath('..'))  # Add the parent directory of `scripts` to the path.\n",
    "\n",
    "from scripts import load_config\n",
    "\n",
    "\n",
    "#------------------------------------------#\n",
    "# Define model settings here.\n",
    "CONFIG_PATH = '/projects/mhpi/leoglonz/dPLT/src/dMG/conf/config_ls.yaml'\n",
    "TEST_SPLIT = 0.2\n",
    "#------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "\n",
    "# Load data\n",
    "with open(config['observations']['train_path'], 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "\n",
    "# Normalize attributes for NN:\n",
    "attrs = data_dict['attributes']\n",
    "for i in range(attrs.shape[-1]):\n",
    "    attrs[:, :, i] = (attrs[:, :, i] - attrs[:, :, i].mean()) \\\n",
    "        / attrs[:, :, i].std()\n",
    "    \n",
    "data_dict['xc_nn_norm'] = attrs\n",
    "\n",
    "\n",
    "# Train-test split + convert to torch tensors\n",
    "train_dataset = {}\n",
    "test_dataset = {}\n",
    "split = int(len(data_dict['forcing']) * (1 - TEST_SPLIT))\n",
    "\n",
    "for key in data_dict.keys():\n",
    "    train_dataset[key] = torch.tensor(\n",
    "        data_dict[key][:split,],\n",
    "        dtype=config['dtype'],\n",
    "        device=config['device'],\n",
    "    )\n",
    "    test_dataset[key] = torch.tensor(\n",
    "        data_dict[key][split:,],\n",
    "        dtype=config['dtype'],\n",
    "        device=config['device'],\n",
    "    )\n",
    "\n",
    "# Reshape to 3d\n",
    "shape = train_dataset['xc_nn_norm'].shape\n",
    "train_dataset['xc_nn_norm'] = train_dataset['xc_nn_norm'].reshape(\n",
    "    shape[0], \n",
    "    shape[1],\n",
    "    shape[2] * shape[3],\n",
    ")\n",
    "\n",
    "shape = test_dataset['xc_nn_norm'].shape\n",
    "test_dataset['xc_nn_norm'] = test_dataset['xc_nn_norm'].reshape(\n",
    "    shape[0], \n",
    "    shape[1],\n",
    "    shape[2] * shape[3],\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load dPL Scheme \n",
    "\n",
    "Physical model (Terzaghi's equation) + LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "from dMG import load_nn_model\n",
    "from src.dMG.models.phy_models.terzaghi import TerzaghiMultiLayer as dPLT\n",
    "importlib.reload(sys.modules['src.dMG.models.phy_models.terzaghi'])\n",
    "importlib.reload(sys.modules['dMG'])\n",
    "\n",
    "\n",
    "model = dPLT(config['dpl_model']['phy_model'], device=config['device'])\n",
    "nn = load_nn_model(\n",
    "    model,\n",
    "    config['dpl_model'],\n",
    "    device=config['device'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Forward Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LstmModel(\n",
      "  (linear_in): Linear(in_features=81, out_features=256, bias=True)\n",
      "  (lstm): Lstm(\n",
      "    (lstm): LSTM(256, 256)\n",
      "  )\n",
      "  (linear_out): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "Input shape torch.Size([109, 16, 81])\n",
      "Output shape torch.Size([109, 16, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/mhpi/leoglonz/dPLT/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
      "  result = _VF.lstm(\n"
     ]
    }
   ],
   "source": [
    "parameters = nn(train_dataset['xc_nn_norm'])\n",
    "\n",
    "print(nn)\n",
    "print(f\"Input shape {train_dataset['xc_nn_norm'].shape}\")\n",
    "print(f\"Output shape {parameters.shape}\")\n",
    "\n",
    "# predictions = model(\n",
    "#     train_dataset,\n",
    "#     parameters,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([109, 16, 8])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([109, 16, 9, 9])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['attributes'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose i have raw_parameters = x_dict['attributes'] with shape [time, sites, layers, parameters, 16] (repeated 16 times). I have another set of parameters which are learned from an lstm, parameters, but this only contains a certain set of parameters learned for a certain set of layers. For example, if i learned the first two parameters for layers 0 and 1, this would look like parameters ~ raw_parameters[time, sites, :2, :2, 16], where the 5th dimension is 16 because I learn the parameter 16 different times and take the average to reduce variance. This is represented by self.nmul in my code. Knowing this, can you modify the parameter handling functions in my model class to overwrite raw_parameters with parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
